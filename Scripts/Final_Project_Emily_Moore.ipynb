{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8476b62d-2825-400c-be4c-35bb3aa81b1c",
   "metadata": {},
   "source": [
    "## Lab 6: Building a Data Lakehouse with the PySpark Structured Streaming Medallion Architecture\n",
    "This lab will help you learn to use many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-2002: Data Systems**. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n",
    "\n",
    "**These include:**\n",
    "- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n",
    "  - Online Transaction Processing Systems (OLTP): *Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n",
    "  - Online Analytical Processing Systems (OLAP): *Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n",
    "- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n",
    "- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n",
    "  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n",
    "- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark/PySpark, Databricks)\n",
    "- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n",
    "\n",
    "## Section I: Prerequisites\n",
    "\n",
    "### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "16355324-fc8a-45b5-b9e5-7b6e8ac814c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pysparkenv/lib/python3.12/site-packages/pyspark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2fdf8-2c35-4152-b60a-5e0ae632f60f",
   "metadata": {},
   "source": [
    "### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "977209d2-77d8-40c6-a497-c5c0958c19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"Millerlite99\",\n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\": \"atlas\",\n",
    "    \"user_name\": \"emilym0723_db_user\",\n",
    "    \"password\": \"RSMC3jpzIh5aez1z\",\n",
    "    \"cluster_name\": \"Cluster0\",\n",
    "    \"cluster_subnet\": \"hytrbnr\",\n",
    "    \"db_name\": \"adventureworks\",\n",
    "    \"collection\": \"M2002\",\n",
    "    \"null_column_threshold\": 0.5\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'Scripts')\n",
    "data_dir = os.path.join(base_dir, 'adventureworks')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "purchase_orders_stream_dir = os.path.join(stream_dir, 'purchase_orders')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"adventureworks_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_sales_order_vw', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_sales_order_vw', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_sales_order_vw', 'gold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a5185-ad2c-4612-9498-0e2ff4431c5b",
   "metadata": {},
   "source": [
    "### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b2fd4b85-6029-439d-a66a-1cab0a9aa760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark AdventureWorks Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c929bad4-705f-4a01-8d9d-bba2d84d115c",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2c09080f-afdc-4969-86cc-0fbd0835faf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"An error occurred: [Errno 66] Directory not empty: '/Users/emilymoore/Downloads/DS-2002-main/04-PySpark/spark-warehouse/adventureworks_dlh.db/fact_sales_order_vw/silver'\""
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa65216-97fc-48d9-b19e-a1f342c53155",
   "metadata": {},
   "source": [
    "### 5.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a416acc5-71ca-40f5-986a-7eeac55b6317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.77:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark AdventureWorks Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1762ffa10>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8701c-0120-4ba2-81cc-e53ba1f08651",
   "metadata": {},
   "source": [
    "### 6.0. Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "484d76e2-3c95-4d0e-9063-37344aae674c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Capstone Project Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Capstone Project');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ae5f1c-7ec7-408c-bc89-822b7a75b859",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "8a3629bc-e7ec-4ed4-9de7-17f648691e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.DS_Store</td>\n",
       "      <td>6148</td>\n",
       "      <td>2025-12-09 21:07:35.477997780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks_address.csv</td>\n",
       "      <td>75516</td>\n",
       "      <td>2025-12-09 17:09:43.810978651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks_customers.json</td>\n",
       "      <td>149540</td>\n",
       "      <td>2025-12-09 20:47:35.457307816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks_employees.json</td>\n",
       "      <td>129366</td>\n",
       "      <td>2025-12-09 20:48:55.777734756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventureworks_products.json</td>\n",
       "      <td>327276</td>\n",
       "      <td>2025-12-09 20:49:25.467095613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adventureworks_territory.csv</td>\n",
       "      <td>935</td>\n",
       "      <td>2025-12-09 17:29:33.271051168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name    size             modification_time\n",
       "0                      .DS_Store    6148 2025-12-09 21:07:35.477997780\n",
       "1     adventureworks_address.csv   75516 2025-12-09 17:09:43.810978651\n",
       "2  adventureworks_customers.json  149540 2025-12-09 20:47:35.457307816\n",
       "3  adventureworks_employees.json  129366 2025-12-09 20:48:55.777734756\n",
       "4   adventureworks_products.json  327276 2025-12-09 20:49:25.467095613\n",
       "5   adventureworks_territory.csv     935 2025-12-09 17:29:33.271051168"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e13a92a-2e5e-46b3-babc-ac10f20d35da",
   "metadata": {},
   "source": [
    "#### Populate the <span style=\"color:darkred\">Address Dimension</span>\n",
    "##### Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "63f9eac0-5c92-4a98-8455-8be2a034d935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emilymoore/Downloads/DS-2002-main/04-PySpark/Scripts/adventureworks/batch/adventureworks_address.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AddressID</th>\n",
       "      <th>AddressLine1</th>\n",
       "      <th>AddressLine2</th>\n",
       "      <th>City</th>\n",
       "      <th>StateProvinceID</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>rowguid</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1970 Napa Ct.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>...</td>\n",
       "      <td>1998-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9833 Mt. Dias Blv.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>...</td>\n",
       "      <td>1999-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AddressID        AddressLine1 AddressLine2     City  StateProvinceID  \\\n",
       "0          1       1970 Napa Ct.         NULL  Bothell               79   \n",
       "1          2  9833 Mt. Dias Blv.         NULL  Bothell               79   \n",
       "\n",
       "  PostalCode rowguid ModifiedDate  \n",
       "0      98011     ...   1998-01-04  \n",
       "1      98011     ...   1999-01-01  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address_csv = os.path.join(batch_dir, 'adventureworks_address.csv')\n",
    "print(address_csv)\n",
    "\n",
    "df_dim_address = spark.read.format('csv').options(header='true', inferSchema='true').load(address_csv)\n",
    "df_dim_address.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e9aedf-9033-403d-a802-b09be878865f",
   "metadata": {},
   "source": [
    "##### Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "18421cba-8e15-44b0-8328-0ff1f1968936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AddressKey</th>\n",
       "      <th>AddressID</th>\n",
       "      <th>AddressLine1</th>\n",
       "      <th>AddressLine2</th>\n",
       "      <th>City</th>\n",
       "      <th>StateProvinceID</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1970 Napa Ct.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>1998-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9833 Mt. Dias Blv.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>1999-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AddressKey  AddressID        AddressLine1 AddressLine2     City  \\\n",
       "0           1          1       1970 Napa Ct.         NULL  Bothell   \n",
       "1           2          2  9833 Mt. Dias Blv.         NULL  Bothell   \n",
       "\n",
       "   StateProvinceID PostalCode ModifiedDate  \n",
       "0               79      98011   1998-01-04  \n",
       "1               79      98011   1999-01-01  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename and remove necessary columns ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_address = df_dim_address.drop(\"rowguid\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_address.createOrReplaceTempView(\"address\")\n",
    "sql_address = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY AddressID) AS AddressKey\n",
    "    FROM address;\n",
    "\"\"\"\n",
    "df_dim_address = spark.sql(sql_address)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['AddressKey', 'AddressID', 'AddressLine1', 'AddressLine2'\n",
    "                   , 'City', 'StateProvinceID', 'PostalCode', 'ModifiedDate']\n",
    "\n",
    "df_dim_address = df_dim_address[ordered_columns]\n",
    "df_dim_address.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a826b-085a-42ce-b9b1-f0991e578eb6",
   "metadata": {},
   "source": [
    "##### Save as the <span style=\"color:darkred\">dim_address</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "5931ed64-99f7-4281-b903-a052c89ce6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_address.write.saveAsTable(f\"{dest_database}.dim_address\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7eacbe-1cad-4ded-9b98-814d140ed8d6",
   "metadata": {},
   "source": [
    "##### Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "414f418e-b9aa-4ef0-a256-b7dd3de1519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          AddressKey|                 int|   NULL|\n",
      "|           AddressID|                 int|   NULL|\n",
      "|        AddressLine1|              string|   NULL|\n",
      "|        AddressLine2|              string|   NULL|\n",
      "|                City|              string|   NULL|\n",
      "|     StateProvinceID|                 int|   NULL|\n",
      "|          PostalCode|              string|   NULL|\n",
      "|        ModifiedDate|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|         dim_address|       |\n",
      "|        Created Time|Tue Dec 09 19:16:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/emily...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AddressKey</th>\n",
       "      <th>AddressID</th>\n",
       "      <th>AddressLine1</th>\n",
       "      <th>AddressLine2</th>\n",
       "      <th>City</th>\n",
       "      <th>StateProvinceID</th>\n",
       "      <th>PostalCode</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1970 Napa Ct.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>1998-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9833 Mt. Dias Blv.</td>\n",
       "      <td>NULL</td>\n",
       "      <td>Bothell</td>\n",
       "      <td>79</td>\n",
       "      <td>98011</td>\n",
       "      <td>1999-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AddressKey  AddressID        AddressLine1 AddressLine2     City  \\\n",
       "0           1          1       1970 Napa Ct.         NULL  Bothell   \n",
       "1           2          2  9833 Mt. Dias Blv.         NULL  Bothell   \n",
       "\n",
       "   StateProvinceID PostalCode ModifiedDate  \n",
       "0               79      98011   1998-01-04  \n",
       "1               79      98011   1999-01-01  "
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_address;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_address LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebca6e1-cf05-44bc-b2c4-29ea6492d5c3",
   "metadata": {},
   "source": [
    "#### Populate the <span style=\"color:darkred\">Territory Dimension</span>\n",
    "##### Use PySpark to Read Data from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "983f5587-5c93-409f-ba2d-d60883a82c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emilymoore/Downloads/DS-2002-main/04-PySpark/Scripts/adventureworks/batch/adventureworks_territory.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>Name</th>\n",
       "      <th>CountryRegionCode</th>\n",
       "      <th>Group</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>CostYTD</th>\n",
       "      <th>CostLastYear</th>\n",
       "      <th>rowguid</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>5.767342e+06</td>\n",
       "      <td>3.298694e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>3.857164e+06</td>\n",
       "      <td>3.607149e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TerritoryID       Name CountryRegionCode          Group      SalesYTD  \\\n",
       "0            1  Northwest                US  North America  5.767342e+06   \n",
       "1            2  Northeast                US  North America  3.857164e+06   \n",
       "\n",
       "   SalesLastYear  CostYTD  CostLastYear rowguid ModifiedDate  \n",
       "0   3.298694e+06        0             0     ...   1998-06-01  \n",
       "1   3.607149e+06        0             0     ...   1998-06-01  "
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "territory_csv = os.path.join(batch_dir, 'adventureworks_territory.csv')\n",
    "print(territory_csv)\n",
    "\n",
    "df_dim_territory = spark.read.format('csv').options(header='true', inferSchema='true').load(territory_csv)\n",
    "df_dim_territory.toPandas().head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce5dcc-59df-4f7e-abf2-604668f77ed4",
   "metadata": {},
   "source": [
    "##### Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7498239a-2320-4496-b594-18fb5fe6279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Territory_Key</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>Name</th>\n",
       "      <th>CountryRegionCode</th>\n",
       "      <th>Group</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>CostYTD</th>\n",
       "      <th>CostLastYear</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>5.767342e+06</td>\n",
       "      <td>3.298694e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>3.857164e+06</td>\n",
       "      <td>3.607149e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Territory_Key  TerritoryID       Name CountryRegionCode          Group  \\\n",
       "0              1            1  Northwest                US  North America   \n",
       "1              2            2  Northeast                US  North America   \n",
       "\n",
       "       SalesYTD  SalesLastYear  CostYTD  CostLastYear ModifiedDate  \n",
       "0  5.767342e+06   3.298694e+06        0             0   1998-06-01  \n",
       "1  3.857164e+06   3.607149e+06        0             0   1998-06-01  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename and remove necessary columns ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_territory = df_dim_territory.drop(\"rowguid\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_territory.createOrReplaceTempView(\"territory\")\n",
    "sql_territory = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY TerritoryID) AS Territory_Key\n",
    "    FROM territory;\n",
    "\"\"\"\n",
    "df_dim_territory = spark.sql(sql_territory)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['Territory_Key', 'TerritoryID', 'Name'\n",
    "                   , 'CountryRegionCode', 'Group', 'SalesYTD', 'SalesLastYear', 'CostYTD',\n",
    "                  'CostLastYear', 'ModifiedDate']\n",
    "\n",
    "df_dim_territory = df_dim_territory[ordered_columns]\n",
    "df_dim_territory.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c325c-2fe6-4c8e-b8c2-aeefb127c3ab",
   "metadata": {},
   "source": [
    "### Save as the <span style=\"color:darkred\">dim_territory</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d688f917-d9df-4bee-bd94-1a6147a5558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_territory.write.saveAsTable(f\"{dest_database}.dim_territory\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb59e9-b2a7-4cd7-b601-9e99857af89e",
   "metadata": {},
   "source": [
    "##### 1.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "e73177f0-7590-491e-ae7e-d5256a11dc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|       Territory_Key|                 int|   NULL|\n",
      "|         TerritoryID|                 int|   NULL|\n",
      "|                Name|              string|   NULL|\n",
      "|   CountryRegionCode|              string|   NULL|\n",
      "|               Group|              string|   NULL|\n",
      "|            SalesYTD|              double|   NULL|\n",
      "|       SalesLastYear|              double|   NULL|\n",
      "|             CostYTD|                 int|   NULL|\n",
      "|        CostLastYear|                 int|   NULL|\n",
      "|        ModifiedDate|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_territory|       |\n",
      "|        Created Time|Tue Dec 09 19:16:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Territory_Key</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>Name</th>\n",
       "      <th>CountryRegionCode</th>\n",
       "      <th>Group</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>CostYTD</th>\n",
       "      <th>CostLastYear</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>5.767342e+06</td>\n",
       "      <td>3.298694e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>US</td>\n",
       "      <td>North America</td>\n",
       "      <td>3.857164e+06</td>\n",
       "      <td>3.607149e+06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Territory_Key  TerritoryID       Name CountryRegionCode          Group  \\\n",
       "0              1            1  Northwest                US  North America   \n",
       "1              2            2  Northeast                US  North America   \n",
       "\n",
       "       SalesYTD  SalesLastYear  CostYTD  CostLastYear ModifiedDate  \n",
       "0  5.767342e+06   3.298694e+06        0             0   1998-06-01  \n",
       "1  3.857164e+06   3.607149e+06        0             0   1998-06-01  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_territory;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_territory LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c1854d-8ea5-421d-b2d7-62109d67581f",
   "metadata": {},
   "source": [
    "### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "f14ae8b9-128b-49c6-9aeb-532dafdafb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "\n",
    "json_files = {\"customers\" : \"adventureworks_customers.json\",\n",
    "              \"employees\" : 'adventureworks_employees.json',\n",
    "              \"products\" : 'adventureworks_products.json'\n",
    "             }\n",
    "\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], batch_dir, json_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e2b966f7-d6e4-4f51-81b8-871099605278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>CustomerType</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>TerritoryID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AW00000001</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AccountNumber  CustomerID CustomerType         ModifiedDate  TerritoryID\n",
       "0    AW00000001           1            S  2004-10-13 11:15:07            1\n",
       "1    AW00000002           2            S  2004-10-13 11:15:07            1"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"customers\"\n",
    "\n",
    "df_dim_customers = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d9440367-9aed-4c68-b779-704cd928f782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerKey  CustomerID  TerritoryID AccountNumber         ModifiedDate\n",
       "0            1           1            1    AW00000001  2004-10-13 11:15:07\n",
       "1            2           2            1    AW00000002  2004-10-13 11:15:07"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename the 'id' column to 'customer_id' ------------------------------------------\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers = df_dim_customers.drop(\"rowguid\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using the SQL Windowing function: ROW_NUMBER() \n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_customers.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY CustomerID) AS CustomerKey\n",
    "    FROM customers;\n",
    "\"\"\"\n",
    "df_dim_customers = spark.sql(sql_customers)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['CustomerKey', 'CustomerID', 'TerritoryID', 'AccountNumber', 'ModifiedDate']\n",
    "\n",
    "df_dim_customers = df_dim_customers[ordered_columns]\n",
    "df_dim_customers.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d3467a6d-8037-4d25-b9c1-18e9bbc413c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customers.write.saveAsTable(f\"{dest_database}.dim_customers\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "64add729-9a97-483c-874e-c5aede8eadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         CustomerKey|                 int|   NULL|\n",
      "|          CustomerID|                 int|   NULL|\n",
      "|         TerritoryID|                 int|   NULL|\n",
      "|       AccountNumber|              string|   NULL|\n",
      "|        ModifiedDate|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_customers|       |\n",
      "|        Created Time|Tue Dec 09 19:16:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/emily...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerKey</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>AccountNumber</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000001</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>AW00000002</td>\n",
       "      <td>2004-10-13 11:15:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerKey  CustomerID  TerritoryID AccountNumber         ModifiedDate\n",
       "0            1           1            1    AW00000001  2004-10-13 11:15:07\n",
       "1            2           2            1    AW00000002  2004-10-13 11:15:07"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customers LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2c122223-74ee-4dd2-b506-8e6fcd85b028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>CurrentFlag</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>NationalIDNumber</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>Title</th>\n",
       "      <th>VacationHours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1972-05-15 00:00:00</td>\n",
       "      <td>1209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>1996-07-31 00:00:00</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>M</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "      <td>14417807</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1977-06-03 00:00:00</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>S</td>\n",
       "      <td>2004-07-31 00:00:00</td>\n",
       "      <td>253022876</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             BirthDate  ContactID CurrentFlag  EmployeeID Gender  \\\n",
       "0  1972-05-15 00:00:00       1209           1           1      M   \n",
       "1  1977-06-03 00:00:00       1030           1           2      M   \n",
       "\n",
       "              HireDate                 LoginID  ManagerID MaritalStatus  \\\n",
       "0  1996-07-31 00:00:00    adventure-works\\guy1       16.0             M   \n",
       "1  1997-02-26 00:00:00  adventure-works\\kevin0        6.0             S   \n",
       "\n",
       "          ModifiedDate NationalIDNumber SalariedFlag  SickLeaveHours  \\\n",
       "0  2004-07-31 00:00:00         14417807            0              30   \n",
       "1  2004-07-31 00:00:00        253022876            0              41   \n",
       "\n",
       "                          Title  VacationHours  \n",
       "0  Production Technician - WC60             21  \n",
       "1           Marketing Assistant             42  "
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"employees\"\n",
    "\n",
    "df_dim_employees = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "fb757e10-4c6f-4210-b9ce-29894a1d30dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeKey</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1209</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1030</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeKey  EmployeeID  ContactID                 LoginID  ManagerID  \\\n",
       "0            1           1       1209    adventure-works\\guy1       16.0   \n",
       "1            2           2       1030  adventure-works\\kevin0        6.0   \n",
       "\n",
       "                          Title  \n",
       "0  Production Technician - WC60  \n",
       "1           Marketing Assistant  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename columns and drop unnecessary ones\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees = df_dim_employees.drop(\"NationalIDNumber\", \"BirthDate\", \"MaritalStatus\", \"Gender\", \"HireDate\"\n",
    "                                        , \"SalariedFlag\", \"VacationHours\", \"SickLeaveHours\", \"CurrentFlag\", \"rowguid\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using ROW_NUMBER()\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_employees.createOrReplaceTempView(\"employees\")\n",
    "sql_employees = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY EmployeeID) AS EmployeeKey\n",
    "    FROM employees;\n",
    "\"\"\"\n",
    "df_dim_employees = spark.sql(sql_employees)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['EmployeeKey', 'EmployeeID', 'ContactID', 'LoginID', 'ManagerID', 'Title']\n",
    "df_dim_employees = df_dim_employees[ordered_columns]\n",
    "df_dim_employees.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "242e4322-4e8a-4279-92b8-0eaa18ee871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employees.write.saveAsTable(f\"{dest_database}.dim_employees\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525fa75-7fca-445f-a586-5d8acf995f6e",
   "metadata": {},
   "source": [
    "##### 2.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "3f390f44-e113-49f3-8ac7-8c864bed2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         EmployeeKey|                 int|   NULL|\n",
      "|          EmployeeID|                 int|   NULL|\n",
      "|           ContactID|                 int|   NULL|\n",
      "|             LoginID|              string|   NULL|\n",
      "|           ManagerID|                 int|   NULL|\n",
      "|               Title|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|       dim_employees|       |\n",
      "|        Created Time|Tue Dec 09 19:16:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/emily...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeKey</th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1209</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1030</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeKey  EmployeeID  ContactID                 LoginID  ManagerID  \\\n",
       "0            1           1       1209    adventure-works\\guy1         16   \n",
       "1            2           2       1030  adventure-works\\kevin0          6   \n",
       "\n",
       "                          Title  \n",
       "0  Production Technician - WC60  \n",
       "1           Marketing Assistant  "
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_employees;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_employees LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11cf43d-c4be-446a-bdf9-73400fab4561",
   "metadata": {},
   "source": [
    "#### Populate the <span style=\"color:darkred\">Products Dimension</span>\n",
    "##### Fetch Data from the New MongoDB <span style=\"color:darkred\">Products</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8c2f01d2-1a8e-445a-9913-549b06c783ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Color</th>\n",
       "      <th>DaysToManufacture</th>\n",
       "      <th>FinishedGoodsFlag</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>MakeFlag</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>ProductModelID</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ProductSubcategoryID</th>\n",
       "      <th>ReorderPoint</th>\n",
       "      <th>SafetyStockLevel</th>\n",
       "      <th>SellStartDate</th>\n",
       "      <th>StandardCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750</td>\n",
       "      <td>1000</td>\n",
       "      <td>1998-06-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>750</td>\n",
       "      <td>1000</td>\n",
       "      <td>1998-06-01 00:00:00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Color  DaysToManufacture FinishedGoodsFlag  ListPrice MakeFlag  \\\n",
       "0  None                  0                 0        0.0        0   \n",
       "1  None                  0                 0        0.0        0   \n",
       "\n",
       "          ModifiedDate             Name  ProductID ProductLine  \\\n",
       "0  2004-03-11 10:01:36  Adjustable Race          1        None   \n",
       "1  2004-03-11 10:01:36     Bearing Ball          2        None   \n",
       "\n",
       "   ProductModelID ProductNumber  ProductSubcategoryID  ReorderPoint  \\\n",
       "0             NaN       AR-5381                   NaN           750   \n",
       "1             NaN       BA-8327                   NaN           750   \n",
       "\n",
       "   SafetyStockLevel        SellStartDate  StandardCost  \n",
       "0              1000  1998-06-01 00:00:00           0.0  \n",
       "1              1000  1998-06-01 00:00:00           0.0  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongodb_args[\"collection\"] = \"products\"\n",
    "\n",
    "df_dim_products = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe339fa3-907d-4e5a-9f72-3cd71e179a84",
   "metadata": {},
   "source": [
    "##### Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8e7dc00b-1241-4463-9d07-35066896cba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>Color</th>\n",
       "      <th>StandardCost</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductKey  ProductID             Name ProductNumber ProductLine Color  \\\n",
       "0           1          1  Adjustable Race       AR-5381        None  None   \n",
       "1           2          2     Bearing Ball       BA-8327        None  None   \n",
       "\n",
       "   StandardCost  ListPrice         ModifiedDate  \n",
       "0           0.0        0.0  2004-03-11 10:01:36  \n",
       "1           0.0        0.0  2004-03-11 10:01:36  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Rename columns and drop unnecessary ones\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products = df_dim_products.drop(\"Size\", \"SizeUnitMeasureCode\", \"WeightUnitMeasureCode\", \"Weight\", \"DaysToManufacture\"\n",
    "                                        , \"Style\", \"Class\", \"ProductSubcategoryID\", \"rowguid\", \"ProductModelID\", \n",
    "                                      \"SellStartDate\", \"SellEndDate\", \"DiscontinuedDate\", \"SafetyStockLevel\", \"ReorderPoint\", \"MakeFlag\",\n",
    "                                      \"FinishedGoodsFlag\")\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Add Primary Key column using ROW_NUMBER()\n",
    "# ----------------------------------------------------------------------------------\n",
    "df_dim_products.createOrReplaceTempView(\"products\")\n",
    "sql_products = f\"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY ProductID) AS ProductKey\n",
    "    FROM products;\n",
    "\"\"\"\n",
    "df_dim_products = spark.sql(sql_products)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# Reorder Columns and display the first two rows in a Pandas dataframe\n",
    "# ----------------------------------------------------------------------------------\n",
    "ordered_columns = ['ProductKey', 'ProductID', 'Name', 'ProductNumber', \"ProductLine\", 'Color', 'StandardCost', 'ListPrice', 'ModifiedDate']\n",
    "df_dim_products = df_dim_products[ordered_columns]\n",
    "df_dim_products.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3fc36-487a-4342-b47f-c2cbc04f27b4",
   "metadata": {},
   "source": [
    "##### 2.4.3. Save as the <span style=\"color:darkred\">dim_products</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "76518f9d-133a-4c8a-969b-6ed014fd028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_products.write.saveAsTable(f\"{dest_database}.dim_products\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9d072-764d-478e-80d7-f5b84f6618b3",
   "metadata": {},
   "source": [
    "##### 2.4.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8b046075-3b8f-45fb-a5ce-235862429603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|          ProductKey|                 int|   NULL|\n",
      "|           ProductID|                 int|   NULL|\n",
      "|                Name|              string|   NULL|\n",
      "|       ProductNumber|              string|   NULL|\n",
      "|         ProductLine|              string|   NULL|\n",
      "|               Color|              string|   NULL|\n",
      "|        StandardCost|              double|   NULL|\n",
      "|           ListPrice|              double|   NULL|\n",
      "|        ModifiedDate|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|  adventureworks_dlh|       |\n",
      "|               Table|        dim_products|       |\n",
      "|        Created Time|Tue Dec 09 19:17:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.7|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/Users/emily...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductKey</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ProductNumber</th>\n",
       "      <th>ProductLine</th>\n",
       "      <th>Color</th>\n",
       "      <th>StandardCost</th>\n",
       "      <th>ListPrice</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Adjustable Race</td>\n",
       "      <td>AR-5381</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Bearing Ball</td>\n",
       "      <td>BA-8327</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004-03-11 10:01:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ProductKey  ProductID             Name ProductNumber ProductLine Color  \\\n",
       "0           1          1  Adjustable Race       AR-5381        None  None   \n",
       "1           2          2     Bearing Ball       BA-8327        None  None   \n",
       "\n",
       "   StandardCost  ListPrice         ModifiedDate  \n",
       "0           0.0        0.0  2004-03-11 10:01:36  \n",
       "1           0.0        0.0  2004-03-11 10:01:36  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_products;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_products LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8947777-e4a6-461f-9490-301ccaf4b06a",
   "metadata": {},
   "source": [
    "### 3.0. Fetch Reference Data from a MySQL Database\n",
    "#### 3.1. Populate the <span style=\"color:darkred\">Date Dimension</span>\n",
    "##### 3.1.1 Fetch data from the <span style=\"color:darkred\">dim_date</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0f4af01d-9f24-4287-87cd-07fc31502958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_dim_date = f\"SELECT * FROM adventureworks.dim_date\"\n",
    "df_dim_date = get_mysql_dataframe(spark, sql_dim_date, **mysql_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be56fc-b6e3-42e1-801d-3474957aee4a",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "868fc437-5f13-447e-8def-1fc663eb05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f64095-6b91-43b4-b96f-20d4fafa9a35",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "a08f8ad8-b44d-4a82-bd4e-d94757964c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            date_key|      int|   NULL|\n",
      "|           full_date|     date|   NULL|\n",
      "|           date_name| char(11)|   NULL|\n",
      "|        date_name_us| char(11)|   NULL|\n",
      "|        date_name_eu| char(11)|   NULL|\n",
      "|         day_of_week|  tinyint|   NULL|\n",
      "|    day_name_of_week| char(10)|   NULL|\n",
      "|        day_of_month|  tinyint|   NULL|\n",
      "|         day_of_year|      int|   NULL|\n",
      "|     weekday_weekend| char(10)|   NULL|\n",
      "|        week_of_year|  tinyint|   NULL|\n",
      "|          month_name| char(10)|   NULL|\n",
      "|       month_of_year|  tinyint|   NULL|\n",
      "|is_last_day_of_month|  char(1)|   NULL|\n",
      "|    calendar_quarter|  tinyint|   NULL|\n",
      "|       calendar_year|      int|   NULL|\n",
      "| calendar_year_month| char(10)|   NULL|\n",
      "|   calendar_year_qtr| char(10)|   NULL|\n",
      "|fiscal_month_of_year|  tinyint|   NULL|\n",
      "|      fiscal_quarter|  tinyint|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>date_name</th>\n",
       "      <th>date_name_us</th>\n",
       "      <th>date_name_eu</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_name_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>weekday_weekend</th>\n",
       "      <th>...</th>\n",
       "      <th>is_last_day_of_month</th>\n",
       "      <th>calendar_quarter</th>\n",
       "      <th>calendar_year</th>\n",
       "      <th>calendar_year_month</th>\n",
       "      <th>calendar_year_qtr</th>\n",
       "      <th>fiscal_month_of_year</th>\n",
       "      <th>fiscal_quarter</th>\n",
       "      <th>fiscal_year</th>\n",
       "      <th>fiscal_year_month</th>\n",
       "      <th>fiscal_year_qtr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000/01/01</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>01/01/2000</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000/01/02</td>\n",
       "      <td>01/02/2000</td>\n",
       "      <td>02/01/2000</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>...</td>\n",
       "      <td>N</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-01</td>\n",
       "      <td>2000Q1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000-07</td>\n",
       "      <td>2000Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key   full_date    date_name date_name_us date_name_eu  day_of_week  \\\n",
       "0  20000101  2000-01-01  2000/01/01   01/01/2000   01/01/2000             7   \n",
       "1  20000102  2000-01-02  2000/01/02   01/02/2000   02/01/2000             1   \n",
       "\n",
       "  day_name_of_week  day_of_month  day_of_year weekday_weekend  ...  \\\n",
       "0       Saturday               1            1      Weekend     ...   \n",
       "1       Sunday                 2            2      Weekend     ...   \n",
       "\n",
       "   is_last_day_of_month calendar_quarter  calendar_year calendar_year_month  \\\n",
       "0                     N                1           2000          2000-01      \n",
       "1                     N                1           2000          2000-01      \n",
       "\n",
       "   calendar_year_qtr  fiscal_month_of_year fiscal_quarter fiscal_year  \\\n",
       "0         2000Q1                         7              3        2000   \n",
       "1         2000Q1                         7              3        2000   \n",
       "\n",
       "   fiscal_year_month  fiscal_year_qtr  \n",
       "0         2000-07          2000Q3      \n",
       "1         2000-07          2000Q3      \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375bf727-5f3e-4589-a4da-d57446a376f3",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "29795432-f447-4c0b-a5a1-cc7d4b4fb008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_address</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_customers</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_employees</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_products</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adventureworks_dlh</td>\n",
       "      <td>dim_territory</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>address</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>employees</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>products</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>territory</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             namespace      tableName  isTemporary\n",
       "0   adventureworks_dlh    dim_address        False\n",
       "1   adventureworks_dlh  dim_customers        False\n",
       "2   adventureworks_dlh       dim_date        False\n",
       "3   adventureworks_dlh  dim_employees        False\n",
       "4   adventureworks_dlh   dim_products        False\n",
       "5   adventureworks_dlh  dim_territory        False\n",
       "6                             address         True\n",
       "7                           customers         True\n",
       "8                           employees         True\n",
       "9                            products         True\n",
       "10                          territory         True"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f158f3-15d4-4b77-8080-c3d1bc68986a",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 6.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Orders</span> Fact Data  \n",
    "#### 6.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "2b506c14-a897-4ac8-a01e-590ddc2e49b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.DS_Store</td>\n",
       "      <td>6148</td>\n",
       "      <td>2025-12-09 21:07:40.494396448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adventureworks_orders_01.json</td>\n",
       "      <td>42484498</td>\n",
       "      <td>2025-12-09 16:58:42.263897657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adventureworks_orders_02.json</td>\n",
       "      <td>41868307</td>\n",
       "      <td>2025-12-09 17:02:42.386663437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adventureworks_orders_03.json</td>\n",
       "      <td>41802302</td>\n",
       "      <td>2025-12-09 17:03:12.437009811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            name      size             modification_time\n",
       "0                      .DS_Store      6148 2025-12-09 21:07:40.494396448\n",
       "1  adventureworks_orders_01.json  42484498 2025-12-09 16:58:42.263897657\n",
       "2  adventureworks_orders_02.json  41868307 2025-12-09 17:02:42.386663437\n",
       "3  adventureworks_orders_03.json  41802302 2025-12-09 17:03:12.437009811"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(orders_stream_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f2bda-d78e-4731-9d4e-10d75d3607ba",
   "metadata": {},
   "source": [
    "#### 6.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Orders Fact table</span> Data\n",
    "##### 6.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "fe375157-dfad-4442-8563-0a05cacecb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\", orders_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e94035a-3bc4-4ba2-abf5-63ca89b4e0f7",
   "metadata": {},
   "source": [
    "##### 6.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3cd60124-791d-4df3-99eb-a5943c5b3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    \n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_bronze)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45b461d-1d9c-4233-aae9-cae37539c0ba",
   "metadata": {},
   "source": [
    "##### 6.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "299479ba-fb32-4260-8585-14f9c47b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: 7d717ee2-0c5d-4e3e-8182-b8916c312393\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': True}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bc62d72c-f857-43c0-8c37-716ef3501064",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1baf025-d6fd-4b69-89d2-dd89c3205496",
   "metadata": {},
   "source": [
    "#### 6.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 6.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "28b1f33c-0ebd-4156-9f14-e8588cab2c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_order_date = df_dim_date.select(col(\"date_key\").alias(\"order_date_key\"), col(\"full_date\").alias(\"order_full_date\"))\n",
    "df_dim_paid_date = df_dim_date.select(col(\"date_key\").alias(\"paid_date_key\"), col(\"full_date\").alias(\"paid_full_date\"))\n",
    "df_dim_shipped_date = df_dim_date.select(col(\"date_key\").alias(\"shipped_date_key\"), col(\"full_date\").alias(\"shipped_full_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7b0437-66a7-4386-9084-4594faab6df0",
   "metadata": {},
   "source": [
    "##### 6.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1f8ea470-0cc3-4530-99de-8cf5beae64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_silver = spark.readStream.format(\"parquet\").load(orders_output_bronze) \\\n",
    "    .join(df_dim_customers, \"CustomerID\") \\\n",
    "    .join(df_dim_employees, \"ContactID\") \\\n",
    "    .join(df_dim_products, \"ProductID\") \\\n",
    "    .join(df_dim_order_date, df_dim_order_date.order_full_date.cast(DateType()) == col(\"OrderDate\").cast(DateType()), \"inner\") \\\n",
    "    .join(df_dim_shipped_date, df_dim_shipped_date.shipped_full_date.cast(DateType()) == col(\"ShipDate\").cast(DateType()), \"left_outer\") \\\n",
    "    .join(df_dim_paid_date, df_dim_paid_date.paid_full_date.cast(DateType()) == col(\"DueDate\").cast(DateType()), \"left_outer\") \\\n",
    "    .select(col(\"order_key\").cast(LongType()), \\\n",
    "            col(\"OrderDate\").cast(LongType()), \\\n",
    "            df_dim_customers.CustomerKey.cast(LongType()), \\\n",
    "            df_dim_employees.EmployeeKey.cast(LongType()), \\\n",
    "            df_dim_products.ProductKey.cast(LongType()), \\\n",
    "            df_dim_order_date.order_date_key.cast(LongType()), \\\n",
    "            df_dim_paid_date.paid_date_key.cast(LongType()), \\\n",
    "            df_dim_shipped_date.shipped_date_key.cast(LongType()), \\\n",
    "            col(\"OrderQty\"),\\\n",
    "            col(\"UnitPrice\"), \\\n",
    "            col(\"ShipRate\"), \\\n",
    "            col(\"TaxAmt\"), \\\n",
    "            col(\"Status\"), \\\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "cce36782-37f7-4a7a-baf8-bc6c9d7e647d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "adcb25db-964b-41ed-bfc8-9ca633c219b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_key: long (nullable = true)\n",
      " |-- OrderDate: long (nullable = true)\n",
      " |-- CustomerKey: long (nullable = false)\n",
      " |-- EmployeeKey: long (nullable = false)\n",
      " |-- ProductKey: long (nullable = false)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- paid_date_key: long (nullable = true)\n",
      " |-- shipped_date_key: long (nullable = true)\n",
      " |-- OrderQty: long (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- ShipRate: double (nullable = true)\n",
      " |-- TaxAmt: double (nullable = true)\n",
      " |-- Status: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6c9fd4-c595-4c38-adb3-c5a3a1f13f53",
   "metadata": {},
   "source": [
    "##### 6.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "145b60d0-0985-4641-822a-7e7995ce0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6c445-16dd-4752-9e46-6ba7e9cef121",
   "metadata": {},
   "source": [
    "##### 6.3.4. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ea1e7be2-fea6-4578-9237-344983c52111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: c87fa999-5ca5-4213-af3b-effa2d68b60e\n",
      "Query Name: orders_silver\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query ID: {orders_silver_query.id}\")\n",
    "print(f\"Query Name: {orders_silver_query.name}\")\n",
    "print(f\"Query Status: {orders_silver_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "1493145c-19b3-4446-84e1-b9a37ecf2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb44a5a-b470-4928-bc73-b5ed9ff8da74",
   "metadata": {},
   "source": [
    "#### 6.4. Create Gold Layer: Perform Aggregations\n",
    "##### 6.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the number of Products sold per Category each Month. The results should include The Month, Product Category and Number of Products sold, sorted by the month number when the orders were placed: e.g., January, February, March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "2b0bb272-a7c1-4f2a-a3d9-5ee5fd6f454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orders_by_product_category_gold = spark.readStream.format(\"parquet\").load(orders_output_silver) \\\n",
    ".join(df_dim_products, \"ProductKey\") \\\n",
    ".join(df_dim_date, df_dim_date.date_key.cast(IntegerType()) == col(\"order_date_key\").cast(IntegerType())) \\\n",
    ".groupBy(\"month_of_year\", \"Name\", \"month_name\") \\\n",
    ".agg(count(\"ProductKey\").alias(\"product_count\")) \\\n",
    ".orderBy(asc(\"month_of_year\"), desc(\"product_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "36930416-660d-4814-a66c-a08d732ba921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_by_product_category_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb8abc8-c889-4ed2-8d37-f15bb1c27ffb",
   "metadata": {},
   "source": [
    "##### 6.4.2. Write the Streaming data to a Parquet File in \"Complete\" mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "0619456b-2217-4a27-b9cf-5d764610a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_orders_by_product_category_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_orders_by_product_quantity\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e88c1d0b-8341-4c5a-9970-c7e2f444feca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b627bd3d-d17b-4ab2-b62a-1b988d10a7c6",
   "metadata": {},
   "source": [
    "##### 6.4.3. Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "c1d4cf7a-de59-4596-a8af-0e84790d7b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month_of_year: byte (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- month_name: string (nullable = true)\n",
      " |-- product_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_orders_by_product_quantity = spark.sql(\"SELECT * FROM fact_orders_by_product_quantity\")\n",
    "df_fact_orders_by_product_quantity.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7c34a4-74ab-45cb-b096-fdd860a00012",
   "metadata": {},
   "source": [
    "##### 6.4.4 Create the Final Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "83ba5a02-ea03-4f9e-99d8-f50327779467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_orders_by_product_quantity_gold_final = df_fact_orders_by_product_quantity \\\n",
    ".select(col(\"month_of_year\").alias(\"Month\"),\n",
    "        col(\"Name\").alias(\"Product\"),\n",
    "        col(\"product_count\").alias(\"Product Count\")) \\\n",
    ".orderBy(asc(\"Month\"), desc(\"Product Count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010c836-3593-4ff5-afb1-f5b29798f3dd",
   "metadata": {},
   "source": [
    "##### 6.4.5. Load the Final Results into a New Table and Display the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "3f7ba782-5dca-4030-b386-04e8f7c5e249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Product</th>\n",
       "      <th>Product Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Month, Product, Product Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_orders_by_product_quantity_gold_final.write.saveAsTable(f\"{dest_database}.fact_orders_by_product_quantity\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_orders_by_product_quantity\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd14eb-1c30-4d29-b5ca-46dadafa5419",
   "metadata": {},
   "source": [
    "### 9.0. Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c6edcbef-aa54-4ca0-aa9f-2b58c8931537",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pysparkenv)",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
